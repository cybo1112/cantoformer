{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch_xla\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from google.cloud import storage\n",
    "import tokenizers\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379612"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set hyperparameters\n",
    "seq_length = 128\n",
    "accum_multipler = 1\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "warmup_ratio = 0.06\n",
    "lr = 5e-4\n",
    "\n",
    "data_size = os.stat(\"/mnt/d/data_masked_%s\"%seq_length).st_size // (batch_size*4)\n",
    "\n",
    "num_batches = int(math.ceil(data_size / batch_size))\n",
    "tot_num_steps   = int(math.ceil((data_size / batch_size / accum_multipler)  * epochs))\n",
    "warmup_steps = int(tot_num_steps * warmup_ratio)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches:     10779\n",
      "data_size:       1379612\n",
      "seq_length:      128\n",
      "lr:              0.0005\n",
      "epochs:          1\n",
      "tot_num_steps:   10779\n",
      "warmup_steps:    646\n"
     ]
    }
   ],
   "source": [
    "print('num_batches:    ', num_batches)\n",
    "print('data_size:      ', data_size)\n",
    "print('seq_length:     ', seq_length)\n",
    "print('lr:             ', lr)\n",
    "print('epochs:         ', epochs)\n",
    "print('tot_num_steps:  ', tot_num_steps)\n",
    "print('warmup_steps:   ', warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='xla', index=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize device\n",
    "import os\n",
    "os.environ['TPU_IP_ADDRESS']= \"10.240.178.50\"\n",
    "os.environ['XRT_TPU_CONFIG'] = \"tpu_worker;0;10.240.178.50:8470\"\n",
    "device = xm.xla_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(vocab_file = 'tokenizer/vocab.txt')\n",
    "tokenizer.add_special_tokens([\"<nl>\"])\n",
    "tokenizer.enable_truncation(max_length=seq_length)\n",
    "tokenizer.enable_padding(length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize data path\n",
    "data_original_fn = \"/mnt/d/data_original_%s\"%seq_length\n",
    "data_masked_fn   = \"/mnt/d/data_masked_%s\"%seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(0, 100000)\n",
    "with open(data_original_fn, \"rb\") as f:\n",
    "    data = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "    \n",
    "with open(data_masked_fn, \"rb\") as f:\n",
    "    data_masked = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] \u001b[34m#\u001b[0m 20 l : å¥½ ç°¡ å–® å›  çˆ² å¼· åœ‹ vs æ—¥ æœ¬ \u001b[34mé¦™\u001b[0m \u001b[34mæ¸¯\u001b[0m \u001b[34mäºº\u001b[0m å’ æ‹ å´‡ æ—¥ é» æœƒ è©± æ—¥ æœ¬ å˜¢ å”” å¥½ ï¼Ÿ [ sosad ] [ sosad ] <nl> # 21 m : d åº¦ ç¸® çª® é–ª çµ å©š ï¼Œ \u001b[34mæƒ³\u001b[0m ä¸€ åœ åŸ· å¤š åƒ å¹¾ èšŠ ï¼Œ å°± æµ \u001b[34mç’°\u001b[0m \u001b[34mä¿\u001b[0m åš è—‰ å£ å†‡ é­š ç¿… é£Ÿ ï¼Œ hiauntie å•¦ ï¼Œ æˆ‘ çœŸ \u001b[34mä¿‚\u001b[0m å”” \u001b[34mæœƒ\u001b[0m \u001b[34mæ¯”\u001b[0m ä½¢ åœ° å¾— é€ ğŸ¤¡ <nl> # 22 f : # 21 ä½  éƒ½ å‚» è±¬ æ—¢ \u001b[34mğŸ¤¡\u001b[0m \u001b[34mğŸ¤¡\u001b[0m ğŸ¤¡ ä½  è¬› åˆ° æ‰€ æœ‰ ç’° ä¿ æ‹ éƒ½ ä¿‚ çª® é–ª å’ ğŸ˜’ æˆ‘ è€ æ¯ \u001b[34må·²\u001b[0m \u001b[34mç¶“\u001b[0m \u001b[34måŒ\u001b[0m æˆ‘ è¬› çµ å©š å”” å¥½ é£Ÿ [SEP] \n",
      "\n",
      "[CLS] \u001b[31m[MASK]\u001b[0m 20 l : å¥½ ç°¡ å–® å›  çˆ² å¼· åœ‹ vs æ—¥ æœ¬ \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m å’ æ‹ å´‡ æ—¥ é» æœƒ è©± æ—¥ æœ¬ å˜¢ å”” å¥½ ï¼Ÿ [ sosad ] [ sosad ] <nl> # 21 m : d åº¦ ç¸® çª® é–ª çµ å©š ï¼Œ \u001b[31m[MASK]\u001b[0m ä¸€ åœ åŸ· å¤š åƒ å¹¾ èšŠ ï¼Œ å°± æµ \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m åš è—‰ å£ å†‡ é­š ç¿… é£Ÿ ï¼Œ hiauntie å•¦ ï¼Œ æˆ‘ çœŸ \u001b[31m[MASK]\u001b[0m å”” \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m ä½¢ åœ° å¾— é€ ğŸ¤¡ <nl> # 22 f : # 21 ä½  éƒ½ å‚» è±¬ æ—¢ \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m ğŸ¤¡ ä½  è¬› åˆ° æ‰€ æœ‰ ç’° ä¿ æ‹ éƒ½ ä¿‚ çª® é–ª å’ ğŸ˜’ æˆ‘ è€ æ¯ \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m æˆ‘ è¬› çµ å©š å”” å¥½ é£Ÿ [SEP] "
     ]
    }
   ],
   "source": [
    "#example\n",
    "from termcolor import colored\n",
    "tensor = torch.zeros(())\n",
    "labels = tensor.new_full(data.shape, -100).int()\n",
    "labels[data!=data_masked] = data[data!=data_masked]\n",
    "\n",
    "attention_mask = torch.where(data!=0, torch.ones_like(data), torch.zeros_like(data))\n",
    "\n",
    "\n",
    "for id, label in zip(data, labels):\n",
    "    if not id:\n",
    "        continue\n",
    "    token = tokenizer.id_to_token(id)\n",
    "    if label >= 0:\n",
    "        token = colored(tokenizer.id_to_token(label), 'blue')\n",
    "    print(token, end=\" \")\n",
    "print()\n",
    "print()\n",
    "for id, label in zip(data_masked, labels):\n",
    "    if not id:\n",
    "        continue\n",
    "    token = tokenizer.id_to_token(id)\n",
    "    if label >= 0:\n",
    "        token = colored(token,'red')\n",
    "    print(token, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "class textDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    def __getitem__(self,i):\n",
    "        with open(data_original_fn, \"rb\") as f:\n",
    "            data = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "\n",
    "        with open(data_masked_fn, \"rb\") as f:\n",
    "            data_masked = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "        \n",
    "        attention_mask = torch.where(data!=0, torch.ones_like(data), torch.zeros_like(data))\n",
    "        \n",
    "        tensor = torch.zeros(())\n",
    "        labels = tensor.new_full(data.shape, -100).int()\n",
    "        labels[data!=data_masked] = data[data!=data_masked]\n",
    "        \n",
    "        unmask_no = int(round(attention_mask.sum().item()*0.15*0.1))\n",
    "        unmask_indices = torch.randint(0,labels.shape[0],(unmask_no,))\n",
    "        labels[unmask_indices] = data[unmask_indices]\n",
    "                      \n",
    "        return data_masked.long(), labels.long(), attention_mask.long(), data.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters for network\n",
    "from transformers import ElectraForMaskedLM, ElectraForPreTraining\n",
    "from transformers import ElectraConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "generator_config = ElectraConfig(\n",
    "    max_position_embeddings=seq_length,\n",
    "    num_hidden_layers=12,\n",
    "    vocab_size=50000,\n",
    "    embedding_size=128,\n",
    "    hidden_size = 64,\n",
    "    intermediate_size = 256,\n",
    "    num_attention_heads=1,\n",
    ")\n",
    "discriminator_config = ElectraConfig(\n",
    "    max_position_embeddings=seq_length,\n",
    "    num_hidden_layers=12,\n",
    "    vocab_size=50000,\n",
    "    embedding_size=128,\n",
    "    hidden_size=256,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    ")\n",
    "\n",
    "generator = ElectraForMaskedLM(config=generator_config)\n",
    "generator.to(device)\n",
    "discriminator = ElectraForPreTraining(config=discriminator_config)\n",
    "discriminator.to(device)\n",
    "discriminator.electra.embeddings = generator.electra.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize dataloader and sampler\n",
    "dataset = textDataset(data_size)\n",
    "sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize optimizer and scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "generator_optimizer = AdamW(\n",
    "    generator.parameters(), betas=(0.9, 0.999), \n",
    "    lr = lr, \n",
    "    weight_decay=0.01)\n",
    "discriminator_optimizer = AdamW(\n",
    "    discriminator.parameters(), betas=(0.9, 0.999), \n",
    "    lr = lr, \n",
    "    weight_decay=0.01)\n",
    "\n",
    "total_steps = len(dataloader) * epochs\n",
    "generator_scheduler = get_linear_schedule_with_warmup(generator_optimizer, \n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = tot_num_steps)\n",
    "discriminator_scheduler = get_linear_schedule_with_warmup(discriminator_optimizer, \n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = tot_num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     1  of  10,779.    Elapsed: 0:03:49.    Generator Loss: 21.684.    Discriminator Loss: 1.401.\n",
      "  Batch     2  of  10,779.    Elapsed: 0:07:53.    Generator Loss: 10.847.    Discriminator Loss: 0.694.\n",
      "  Batch     3  of  10,779.    Elapsed: 0:19:42.    Generator Loss: 10.851.    Discriminator Loss: 0.689.\n",
      "  Batch     4  of  10,779.    Elapsed: 0:31:49.    Generator Loss: 10.836.    Discriminator Loss: 0.683.\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "total_t0 = time.time()\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    generator_train_loss = 0\n",
    "    discriminator_train_loss = 0\n",
    "\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    generator.zero_grad()\n",
    "    discriminator.zero_grad()\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        \n",
    "        #generator\n",
    "        generator_input = batch[0].to(device)\n",
    "        generator_labels = batch[1].to(device)\n",
    "        generator_mask = batch[2].to(device)\n",
    "        generator_original = batch[3].to(device)\n",
    "        \n",
    "        generator_loss, generator_scores = generator(generator_input, attention_mask=generator_mask, labels=generator_labels)\n",
    "        generator_loss = generator_loss.mean()\n",
    "        generator_train_loss += generator_loss.item()\n",
    "        generator_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "        \n",
    "        #discriminator\n",
    "        discriminator_input = torch.where(generator_labels>=0, torch.argmax(generator_scores,dim=2), generator_original)\n",
    "        discriminator_labels = torch.where(discriminator_input==generator_original, \n",
    "                                           torch.zeros_like(generator_original), torch.ones_like(generator_original))\n",
    "        discriminator_mask = generator_mask\n",
    "        \n",
    "        \n",
    "        discriminator_loss, discriminator_scores = discriminator(discriminator_input, \n",
    "                                                    attention_mask=discriminator_mask, labels=discriminator_labels)\n",
    "        discriminator_loss = discriminator_loss.mean()\n",
    "        discriminator_train_loss += discriminator_loss.item()\n",
    "        discriminator_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "        \n",
    "        if step % accum_multipler == 0 and (accum_multipler == 1 or step != 0):\n",
    "            xm.optimizer_step(generator_optimizer)\n",
    "            generator_scheduler.step()\n",
    "            xm.optimizer_step(discriminator_optimizer)\n",
    "            discriminator_scheduler.step()\n",
    "            generator.zero_grad()\n",
    "            discriminator.zero_grad()\n",
    "        \n",
    "        if step % 1 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            xm.master_print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.    Generator Loss: {:.3f}.    Discriminator Loss: {:.3f}.'\n",
    "                  .format(step, \n",
    "                          len(dataloader), \n",
    "                          elapsed, \n",
    "                          generator_train_loss/1, discriminator_train_loss/1))\n",
    "\n",
    "            generator_train_loss = 0\n",
    "            discriminator_train_loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
