{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch_xla\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from google.cloud import storage\n",
    "import tokenizers\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379612"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set hyperparameters\n",
    "seq_length = 128\n",
    "accum_multipler = 1\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "warmup_ratio = 0.06\n",
    "lr = 5e-4\n",
    "\n",
    "data_size = os.stat(\"/mnt/d/data_masked_%s\"%seq_length).st_size // (batch_size*4)\n",
    "\n",
    "num_batches = int(math.ceil(data_size / batch_size))\n",
    "tot_num_steps   = int(math.ceil((data_size / batch_size / accum_multipler)  * epochs))\n",
    "warmup_steps = int(tot_num_steps * warmup_ratio)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches:     10779\n",
      "data_size:       1379612\n",
      "seq_length:      128\n",
      "lr:              0.0005\n",
      "epochs:          1\n",
      "tot_num_steps:   10779\n",
      "warmup_steps:    646\n"
     ]
    }
   ],
   "source": [
    "print('num_batches:    ', num_batches)\n",
    "print('data_size:      ', data_size)\n",
    "print('seq_length:     ', seq_length)\n",
    "print('lr:             ', lr)\n",
    "print('epochs:         ', epochs)\n",
    "print('tot_num_steps:  ', tot_num_steps)\n",
    "print('warmup_steps:   ', warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='xla', index=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize device\n",
    "import os\n",
    "os.environ['TPU_IP_ADDRESS']= \"10.240.178.50\"\n",
    "os.environ['XRT_TPU_CONFIG'] = \"tpu_worker;0;10.240.178.50:8470\"\n",
    "device = xm.xla_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(vocab_file = 'tokenizer/vocab.txt')\n",
    "tokenizer.add_special_tokens([\"<nl>\"])\n",
    "tokenizer.enable_truncation(max_length=seq_length)\n",
    "tokenizer.enable_padding(length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize data path\n",
    "data_original_fn = \"/mnt/d/data_original_%s\"%seq_length\n",
    "data_masked_fn   = \"/mnt/d/data_masked_%s\"%seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(0, 100000)\n",
    "with open(data_original_fn, \"rb\") as f:\n",
    "    data = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "    \n",
    "with open(data_masked_fn, \"rb\") as f:\n",
    "    data_masked = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] \u001b[34m#\u001b[0m 20 l : 好 簡 單 因 爲 強 國 vs 日 本 \u001b[34m香\u001b[0m \u001b[34m港\u001b[0m \u001b[34m人\u001b[0m 咁 拎 崇 日 點 會 話 日 本 嘢 唔 好 ？ [ sosad ] [ sosad ] <nl> # 21 m : d 度 縮 窮 閪 結 婚 ， \u001b[34m想\u001b[0m 一 圍 執 多 千 幾 蚊 ， 就 搵 \u001b[34m環\u001b[0m \u001b[34m保\u001b[0m 做 藉 口 冇 魚 翅 食 ， hiauntie 啦 ， 我 真 \u001b[34m係\u001b[0m 唔 \u001b[34m會\u001b[0m \u001b[34m比\u001b[0m 佢 地 得 逞 🤡 <nl> # 22 f : # 21 你 都 傻 豬 既 \u001b[34m🤡\u001b[0m \u001b[34m🤡\u001b[0m 🤡 你 講 到 所 有 環 保 拎 都 係 窮 閪 咁 😒 我 老 母 \u001b[34m已\u001b[0m \u001b[34m經\u001b[0m \u001b[34m同\u001b[0m 我 講 結 婚 唔 好 食 [SEP] \n",
      "\n",
      "[CLS] \u001b[31m[MASK]\u001b[0m 20 l : 好 簡 單 因 爲 強 國 vs 日 本 \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m 咁 拎 崇 日 點 會 話 日 本 嘢 唔 好 ？ [ sosad ] [ sosad ] <nl> # 21 m : d 度 縮 窮 閪 結 婚 ， \u001b[31m[MASK]\u001b[0m 一 圍 執 多 千 幾 蚊 ， 就 搵 \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m 做 藉 口 冇 魚 翅 食 ， hiauntie 啦 ， 我 真 \u001b[31m[MASK]\u001b[0m 唔 \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m 佢 地 得 逞 🤡 <nl> # 22 f : # 21 你 都 傻 豬 既 \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m 🤡 你 講 到 所 有 環 保 拎 都 係 窮 閪 咁 😒 我 老 母 \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m 我 講 結 婚 唔 好 食 [SEP] "
     ]
    }
   ],
   "source": [
    "#example\n",
    "from termcolor import colored\n",
    "tensor = torch.zeros(())\n",
    "labels = tensor.new_full(data.shape, -100).int()\n",
    "labels[data!=data_masked] = data[data!=data_masked]\n",
    "\n",
    "attention_mask = torch.where(data!=0, torch.ones_like(data), torch.zeros_like(data))\n",
    "\n",
    "\n",
    "for id, label in zip(data, labels):\n",
    "    if not id:\n",
    "        continue\n",
    "    token = tokenizer.id_to_token(id)\n",
    "    if label >= 0:\n",
    "        token = colored(tokenizer.id_to_token(label), 'blue')\n",
    "    print(token, end=\" \")\n",
    "print()\n",
    "print()\n",
    "for id, label in zip(data_masked, labels):\n",
    "    if not id:\n",
    "        continue\n",
    "    token = tokenizer.id_to_token(id)\n",
    "    if label >= 0:\n",
    "        token = colored(token,'red')\n",
    "    print(token, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataset\n",
    "class textDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    def __getitem__(self,i):\n",
    "        with open(data_original_fn, \"rb\") as f:\n",
    "            data = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "\n",
    "        with open(data_masked_fn, \"rb\") as f:\n",
    "            data_masked = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "        \n",
    "        attention_mask = torch.where(data!=0, torch.ones_like(data), torch.zeros_like(data))\n",
    "        \n",
    "        tensor = torch.zeros(())\n",
    "        labels = tensor.new_full(data.shape, -100).int()\n",
    "        labels[data!=data_masked] = data[data!=data_masked]\n",
    "        \n",
    "        unmask_no = int(round(attention_mask.sum().item()*0.15*0.1))\n",
    "        unmask_indices = torch.randint(0,labels.shape[0],(unmask_no,))\n",
    "        labels[unmask_indices] = data[unmask_indices]\n",
    "                      \n",
    "        return data_masked.long(), labels.long(), attention_mask.long(), data.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyperparameters for network\n",
    "from transformers import ElectraForMaskedLM, ElectraForPreTraining\n",
    "from transformers import ElectraConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "generator_config = ElectraConfig(\n",
    "    max_position_embeddings=seq_length,\n",
    "    num_hidden_layers=12,\n",
    "    vocab_size=50000,\n",
    "    embedding_size=128,\n",
    "    hidden_size = 64,\n",
    "    intermediate_size = 256,\n",
    "    num_attention_heads=1,\n",
    ")\n",
    "discriminator_config = ElectraConfig(\n",
    "    max_position_embeddings=seq_length,\n",
    "    num_hidden_layers=12,\n",
    "    vocab_size=50000,\n",
    "    embedding_size=128,\n",
    "    hidden_size=256,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    ")\n",
    "\n",
    "generator = ElectraForMaskedLM(config=generator_config)\n",
    "generator.to(device)\n",
    "discriminator = ElectraForPreTraining(config=discriminator_config)\n",
    "discriminator.to(device)\n",
    "discriminator.electra.embeddings = generator.electra.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize dataloader and sampler\n",
    "dataset = textDataset(data_size)\n",
    "sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "      dataset,\n",
    "      num_replicas=xm.xrt_world_size(),\n",
    "      rank=xm.get_ordinal(),\n",
    "      shuffle=True)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize optimizer and scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "generator_optimizer = AdamW(\n",
    "    generator.parameters(), betas=(0.9, 0.999), \n",
    "    lr = lr, \n",
    "    weight_decay=0.01)\n",
    "discriminator_optimizer = AdamW(\n",
    "    discriminator.parameters(), betas=(0.9, 0.999), \n",
    "    lr = lr, \n",
    "    weight_decay=0.01)\n",
    "\n",
    "total_steps = len(dataloader) * epochs\n",
    "generator_scheduler = get_linear_schedule_with_warmup(generator_optimizer, \n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = tot_num_steps)\n",
    "discriminator_scheduler = get_linear_schedule_with_warmup(discriminator_optimizer, \n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = tot_num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 1 ========\n",
      "Training...\n",
      "  Batch     1  of  10,779.    Elapsed: 0:03:49.    Generator Loss: 21.684.    Discriminator Loss: 1.401.\n",
      "  Batch     2  of  10,779.    Elapsed: 0:07:53.    Generator Loss: 10.847.    Discriminator Loss: 0.694.\n",
      "  Batch     3  of  10,779.    Elapsed: 0:19:42.    Generator Loss: 10.851.    Discriminator Loss: 0.689.\n",
      "  Batch     4  of  10,779.    Elapsed: 0:31:49.    Generator Loss: 10.836.    Discriminator Loss: 0.683.\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "total_t0 = time.time()\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    generator_train_loss = 0\n",
    "    discriminator_train_loss = 0\n",
    "\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    generator.zero_grad()\n",
    "    discriminator.zero_grad()\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        \n",
    "        #generator\n",
    "        generator_input = batch[0].to(device)\n",
    "        generator_labels = batch[1].to(device)\n",
    "        generator_mask = batch[2].to(device)\n",
    "        generator_original = batch[3].to(device)\n",
    "        \n",
    "        generator_loss, generator_scores = generator(generator_input, attention_mask=generator_mask, labels=generator_labels)\n",
    "        generator_loss = generator_loss.mean()\n",
    "        generator_train_loss += generator_loss.item()\n",
    "        generator_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "        \n",
    "        #discriminator\n",
    "        discriminator_input = torch.where(generator_labels>=0, torch.argmax(generator_scores,dim=2), generator_original)\n",
    "        discriminator_labels = torch.where(discriminator_input==generator_original, \n",
    "                                           torch.zeros_like(generator_original), torch.ones_like(generator_original))\n",
    "        discriminator_mask = generator_mask\n",
    "        \n",
    "        \n",
    "        discriminator_loss, discriminator_scores = discriminator(discriminator_input, \n",
    "                                                    attention_mask=discriminator_mask, labels=discriminator_labels)\n",
    "        discriminator_loss = discriminator_loss.mean()\n",
    "        discriminator_train_loss += discriminator_loss.item()\n",
    "        discriminator_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "        \n",
    "        if step % accum_multipler == 0 and (accum_multipler == 1 or step != 0):\n",
    "            xm.optimizer_step(generator_optimizer)\n",
    "            generator_scheduler.step()\n",
    "            xm.optimizer_step(discriminator_optimizer)\n",
    "            discriminator_scheduler.step()\n",
    "            generator.zero_grad()\n",
    "            discriminator.zero_grad()\n",
    "        \n",
    "        if step % 1 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            xm.master_print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.    Generator Loss: {:.3f}.    Discriminator Loss: {:.3f}.'\n",
    "                  .format(step, \n",
    "                          len(dataloader), \n",
    "                          elapsed, \n",
    "                          generator_train_loss/1, discriminator_train_loss/1))\n",
    "\n",
    "            generator_train_loss = 0\n",
    "            discriminator_train_loss = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
