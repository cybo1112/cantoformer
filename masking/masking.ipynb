{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from google.cloud import storage\n",
    "import tokenizers\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import numpy as np\n",
    "import random\n",
    "import jieba\n",
    "import logging\n",
    "logging.getLogger(\"jieba\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(vocab_file = '../tokenizer/vocab.txt')\n",
    "tokenizer.add_special_tokens([\"<nl>\"])\n",
    "tokenizer.enable_truncation(max_length=seq_length)\n",
    "tokenizer.enable_padding(length=seq_length)\n",
    "client = storage.Client()\n",
    "blobs = []\n",
    "size = 0\n",
    "for blob in client.list_blobs('tfrc-tfrc', prefix='public_model/corpus/'):\n",
    "    if(blob.name.endswith('.txt')):\n",
    "        blobs.append(blob)\n",
    "        \n",
    "sub_blobs = blobs[245:255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator_gen(generator, handler=None, parallel = False):\n",
    "    try:\n",
    "        import gc\n",
    "        import multiprocessing as multiprocessing\n",
    "        if parallel:\n",
    "            cpu_count = multiprocessing.cpu_count()\n",
    "            pool = multiprocessing.Pool(cpu_count)\n",
    "\n",
    "        if handler is not None:\n",
    "            \n",
    "\n",
    "            for e in (pool.imap(handler,generator) if parallel else iter(handler(e) for e in generator)):\n",
    "                if e:\n",
    "                    if isinstance(e, list):\n",
    "                        for e in e:\n",
    "                            yield e\n",
    "                    else:\n",
    "                        yield e\n",
    "                    \n",
    "        else:\n",
    "            for e in generator:\n",
    "                if e:\n",
    "                    if isinstance(e, list):\n",
    "                        for e in e:\n",
    "                            yield e\n",
    "                    else:\n",
    "                        yield e\n",
    "    finally:\n",
    "        if parallel:\n",
    "            try:\n",
    "                pool.terminate()\n",
    "            except:\n",
    "                pass\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mask_ids(encoding, words, offset):\n",
    "    mask_no = int(round((len(encoding.ids) - (np.array(encoding.ids)<=5).sum()).item()*0.15))\n",
    "    words_nospace = [word for word in words if word[0]!=' ']\n",
    "    if(mask_no > len(words_nospace)):\n",
    "        return None\n",
    "    sample = random.sample(words_nospace, k=mask_no)\n",
    "    masked_ids = np.array(encoding.ids)\n",
    "    masked_words = 0\n",
    "    \n",
    "    for word in sample:\n",
    "        start_index = [ind for ind, i in enumerate(encoding.offsets) if i[0] == (word[1]+offset)]\n",
    "        end_index = [ind for ind, i in enumerate(encoding.offsets) if i[1] == (word[2]+offset)]\n",
    "        if len(start_index)==0 or len(end_index)==0:\n",
    "            continue\n",
    "        else:\n",
    "            start_index, end_index = start_index[0], end_index[0]\n",
    "            if start_index == 0:\n",
    "                start_index += 1\n",
    "            masked_words += end_index - start_index + 1\n",
    "            if random.random()<=0.1:\n",
    "                masked_ids[start_index:end_index+1] = np.random.randint(6,50000,size = end_index - start_index + 1)\n",
    "            elif random.random() <=0.8:\n",
    "                masked_ids[start_index:end_index+1] = 4\n",
    "        if masked_words > mask_no:\n",
    "            break\n",
    "    if np.count_nonzero(masked_ids == 4) == 0:\n",
    "        return None\n",
    "    return masked_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start tokenizing file public_model/corpus/golden_old2_吹水台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_吹水台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_吹水台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_吹水台000001.txt - total size: 301158\n",
      "start tokenizing file public_model/corpus/golden_old2_娛樂台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_娛樂台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_娛樂台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_娛樂台000001.txt - total size: 425882\n",
      "start tokenizing file public_model/corpus/golden_old2_學術台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_學術台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_學術台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_學術台000001.txt - total size: 594656\n",
      "start tokenizing file public_model/corpus/golden_old2_寵物台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_寵物台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_寵物台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_寵物台000001.txt - total size: 594656\n",
      "start tokenizing file public_model/corpus/golden_old2_影視台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_影視台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_影視台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_影視台000001.txt - total size: 655844\n",
      "start tokenizing file public_model/corpus/golden_old2_感情台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_感情台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_感情台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_感情台000001.txt - total size: 866725\n",
      "start tokenizing file public_model/corpus/golden_old2_攝影台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_攝影台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_攝影台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_攝影台000001.txt - total size: 879234\n",
      "start tokenizing file public_model/corpus/golden_old2_旅遊台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_旅遊台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_旅遊台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_旅遊台000001.txt - total size: 897043\n",
      "start tokenizing file public_model/corpus/golden_old2_時事台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_時事台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_時事台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_時事台000001.txt - total size: 1379612\n",
      "start tokenizing file public_model/corpus/golden_old2_校園台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old2_校園台000001.txt\n",
      "start masking file public_model/corpus/golden_old2_校園台000001.txt\n",
      "finish masking file public_model/corpus/golden_old2_校園台000001.txt - total size: 1379612\n"
     ]
    }
   ],
   "source": [
    "#generator ver\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "total_size = 0\n",
    "with open(\"/mnt/d/data_original_%s\"%seq_length, \"wb\") as f, open(\"/mnt/d/data_masked_%s\"%seq_length, \"wb\") as m:\n",
    "    for count, blob in enumerate(sub_blobs):\n",
    "        data = blob.download_as_string()\n",
    "        data = data.decode(\"utf-8\")\n",
    "        data = data.split(\"\\n\\n\")\n",
    "        flat_data = []\n",
    "        for line in data:\n",
    "            if len(line) > 100000:\n",
    "                line = [line[i:i+100000] for i in range(0, len(line), 100000)]\n",
    "                flat_data.extend(line)\n",
    "            else:\n",
    "                flat_data.append(line)\n",
    "        data = flat_data\n",
    "        print(f\"start tokenizing file {blob.name}\")\n",
    "        encoded = tokenizer.encode_batch(data)\n",
    "        print(f\"finish tokenizing file {blob.name}\")\n",
    "        # Prepare something for worker to do\n",
    "        def generator():\n",
    "            index = 0\n",
    "            for item in encoded:\n",
    "                yield(item, index)\n",
    "                index += 1\n",
    "\n",
    "        # Actual Work\n",
    "        def worker(item):\n",
    "            size = 0\n",
    "            ids, masked_ids = [], []\n",
    "            index = item[1]\n",
    "            item = item[0]\n",
    "\n",
    "            min_index = item.offsets[1][0] #get original index of first word in encoded segment\n",
    "            max_index = max(item.offsets)[1] #get original index of last word in encoded segment\n",
    "            words = list(jieba.tokenize(data[index][min_index:max_index]))\n",
    "            arr = np.array(item.ids, dtype=np.int32)\n",
    "            if(np.count_nonzero(arr) > 15):\n",
    "                masked_id = mask_ids(item, words, min_index)\n",
    "                if masked_id is not None:\n",
    "                    ids.append(arr)\n",
    "                    masked_ids.append(np.array(masked_id, dtype=np.int32))\n",
    "                    size += 1\n",
    "                \n",
    "            for overflowing in item.overflowing:\n",
    "                min_index = overflowing.offsets[1][0]\n",
    "                max_index = max(overflowing.offsets)[1]\n",
    "                words = list(jieba.tokenize(data[index][min_index:max_index]))\n",
    "                arr = np.array(overflowing.ids, dtype=np.int32)\n",
    "                if(np.count_nonzero(arr) > 15):\n",
    "                    masked_id = mask_ids(overflowing, words, min_index)\n",
    "                    if masked_id is not None:\n",
    "                        ids.append(arr)\n",
    "                        masked_ids.append(np.array(masked_id, dtype=np.int32))\n",
    "                        size += 1\n",
    "            \n",
    "            return ids, masked_ids, size\n",
    "            \n",
    "\n",
    "        g = generator()\n",
    "        print(f\"start masking file {blob.name}\")\n",
    "        for ids, masked_ids, size in iterator_gen(g, worker, parallel=True):\n",
    "            for i in masked_ids:\n",
    "                m.write(i)\n",
    "            for i in ids:\n",
    "                f.write(i)\n",
    "            total_size += size\n",
    "        print(f\"finish masking file {blob.name} - total size: {total_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
