{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from google.cloud import storage\n",
    "import tokenizers\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import numpy as np\n",
    "import random\n",
    "import jieba\n",
    "import logging\n",
    "logging.getLogger(\"jieba\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(vocab_file = '../tokenizer/vocab.txt')\n",
    "tokenizer.add_special_tokens([\"<nl>\"])\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "tokenizer.enable_padding(length=512)\n",
    "client = storage.Client()\n",
    "blobs = []\n",
    "size = 0\n",
    "for blob in client.list_blobs('tfrc-tfrc', prefix='public_model/corpus/'):\n",
    "    if(blob.name.endswith('.txt')):\n",
    "        blobs.append(blob)\n",
    "        \n",
    "sub_blobs = random.sample(blobs,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator_gen(generator, handler=None, parallel = False):\n",
    "    try:\n",
    "        import gc\n",
    "        import multiprocessing as multiprocessing\n",
    "        if parallel:\n",
    "            cpu_count = multiprocessing.cpu_count()\n",
    "            pool = multiprocessing.Pool(cpu_count)\n",
    "\n",
    "        if handler is not None:\n",
    "            \n",
    "\n",
    "            for e in (pool.imap(handler,generator) if parallel else iter(handler(e) for e in generator)):\n",
    "                if e:\n",
    "                    if isinstance(e, list):\n",
    "                        for e in e:\n",
    "                            yield e\n",
    "                    else:\n",
    "                        yield e\n",
    "                    \n",
    "        else:\n",
    "            for e in generator:\n",
    "                if e:\n",
    "                    if isinstance(e, list):\n",
    "                        for e in e:\n",
    "                            yield e\n",
    "                    else:\n",
    "                        yield e\n",
    "    finally:\n",
    "        if parallel:\n",
    "            try:\n",
    "                pool.terminate()\n",
    "            except:\n",
    "                pass\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mask_ids(encoding, words, offset):\n",
    "    mask_no = int(round((len(encoding.ids) - (np.array(encoding.ids)<=5).sum()).item()*0.15))\n",
    "    words_nospace = [word for word in words if word[0]!=' ']\n",
    "    if(mask_no > len(words_nospace)):\n",
    "        return None\n",
    "    sample = random.sample(words_nospace, k=mask_no)\n",
    "    masked_ids = np.array(encoding.ids)\n",
    "    masked_words = 0\n",
    "    \n",
    "    for word in sample:\n",
    "        start_index = [ind for ind, i in enumerate(encoding.offsets) if i[0] == (word[1]+offset)]\n",
    "        end_index = [ind for ind, i in enumerate(encoding.offsets) if i[1] == (word[2]+offset)]\n",
    "        if len(start_index)==0 or len(end_index)==0:\n",
    "            continue\n",
    "        else:\n",
    "            start_index, end_index = start_index[0], end_index[0]\n",
    "            if start_index == 0:\n",
    "                start_index += 1\n",
    "            masked_words += end_index - start_index + 1\n",
    "            if random.random()<=0.1:\n",
    "                masked_ids[start_index:end_index+1] = np.random.randint(6,50000,size = end_index - start_index + 1)\n",
    "            elif random.random() <=0.8:\n",
    "                masked_ids[start_index:end_index+1] = 4\n",
    "        if masked_words > mask_no:\n",
    "            break\n",
    "    if np.count_nonzero(masked_ids == 4) == 0:\n",
    "        return None\n",
    "    return masked_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start tokenizing file public_model/corpus/medium000030.txt\n",
      "finish tokenizing file public_model/corpus/medium000030.txt\n",
      "start masking file public_model/corpus/medium000030.txt\n",
      "finish masking file public_model/corpus/medium000030.txt - total size: 73798\n",
      "start tokenizing file public_model/corpus/dcard000017.txt\n",
      "finish tokenizing file public_model/corpus/dcard000017.txt\n",
      "start masking file public_model/corpus/dcard000017.txt\n",
      "finish masking file public_model/corpus/dcard000017.txt - total size: 313379\n",
      "start tokenizing file public_model/corpus/golden_old5_時事台000001.txt\n",
      "finish tokenizing file public_model/corpus/golden_old5_時事台000001.txt\n",
      "start masking file public_model/corpus/golden_old5_時事台000001.txt\n",
      "finish masking file public_model/corpus/golden_old5_時事台000001.txt - total size: 507160\n",
      "start tokenizing file public_model/corpus/wiki_en000034.txt\n",
      "finish tokenizing file public_model/corpus/wiki_en000034.txt\n",
      "start masking file public_model/corpus/wiki_en000034.txt\n",
      "finish masking file public_model/corpus/wiki_en000034.txt - total size: 583543\n",
      "start tokenizing file public_model/corpus/reddit000043.txt\n",
      "finish tokenizing file public_model/corpus/reddit000043.txt\n",
      "start masking file public_model/corpus/reddit000043.txt\n",
      "finish masking file public_model/corpus/reddit000043.txt - total size: 648168\n"
     ]
    }
   ],
   "source": [
    "#generator ver\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "total_size = 0\n",
    "with open(\"/mnt/d/data_original\", \"wb\") as f, open(\"/mnt/d/data_masked\", \"wb\") as m:\n",
    "    for count, blob in enumerate(sub_blobs):\n",
    "        data = blob.download_as_string()\n",
    "        data = data.decode(\"utf-8\")\n",
    "        data = data.split(\"\\n\\n\")\n",
    "        flat_data = []\n",
    "        for line in data:\n",
    "            if len(line) > 100000:\n",
    "                line = [line[i:i+100000] for i in range(0, len(line), 100000)]\n",
    "                flat_data.extend(line)\n",
    "            else:\n",
    "                flat_data.append(line)\n",
    "        data = flat_data\n",
    "        print(f\"start tokenizing file {blob.name}\")\n",
    "        encoded = tokenizer.encode_batch(data)\n",
    "        print(f\"finish tokenizing file {blob.name}\")\n",
    "        # Prepare something for worker to do\n",
    "        def generator():\n",
    "            index = 0\n",
    "            for item in encoded:\n",
    "                yield(item, index)\n",
    "                index += 1\n",
    "\n",
    "        # Actual Work\n",
    "        def worker(item):\n",
    "            size = 0\n",
    "            ids, masked_ids = [], []\n",
    "            index = item[1]\n",
    "            item = item[0]\n",
    "\n",
    "            min_index = item.offsets[1][0] #get original index of first word in encoded segment\n",
    "            max_index = max(item.offsets)[1] #get original index of last word in encoded segment\n",
    "            words = list(jieba.tokenize(data[index][min_index:max_index]))\n",
    "            arr = np.array(item.ids, dtype=np.int32)\n",
    "            if(np.count_nonzero(arr) > 15):\n",
    "                masked_id = mask_ids(item, words, min_index)\n",
    "                if masked_id is not None:\n",
    "                    ids.append(arr)\n",
    "                    masked_ids.append(np.array(masked_id, dtype=np.int32))\n",
    "                    size += 1\n",
    "                \n",
    "            for overflowing in item.overflowing:\n",
    "                min_index = overflowing.offsets[1][0]\n",
    "                max_index = max(overflowing.offsets)[1]\n",
    "                words = list(jieba.tokenize(data[index][min_index:max_index]))\n",
    "                arr = np.array(overflowing.ids, dtype=np.int32)\n",
    "                if(np.count_nonzero(arr) > 15):\n",
    "                    masked_id = mask_ids(overflowing, words, min_index)\n",
    "                    if masked_id is not None:\n",
    "                        ids.append(arr)\n",
    "                        masked_ids.append(np.array(masked_id, dtype=np.int32))\n",
    "                        size += 1\n",
    "            \n",
    "            return ids, masked_ids, size\n",
    "            \n",
    "\n",
    "        g = generator()\n",
    "        print(f\"start masking file {blob.name}\")\n",
    "        for ids, masked_ids, size in iterator_gen(g, worker, parallel=True):\n",
    "            for i in masked_ids:\n",
    "                m.write(i)\n",
    "            for i in ids:\n",
    "                f.write(i)\n",
    "            total_size += size\n",
    "        print(f\"finish masking file {blob.name} - total size: {total_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 29 12:20:16 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.87.01    Driver Version: 418.87.01    CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P0    58W / 300W |   2854MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   40C    P0    58W / 300W |   1328MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   41C    P0    58W / 300W |   1328MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   47C    P0    62W / 300W |   1328MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  Off  | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   39C    P0    58W / 300W |   1328MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  Off  | 00000000:00:09.0 Off |                    0 |\n",
      "| N/A   41C    P0    57W / 300W |   1328MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  Off  | 00000000:00:0A.0 Off |                    0 |\n",
      "| N/A   39C    P0    57W / 300W |   1328MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  Off  | 00000000:00:0B.0 Off |                    0 |\n",
      "| N/A   41C    P0    59W / 300W |   1328MiB / 16130MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     18373      C   /opt/conda/bin/python                       2843MiB |\n",
      "|    1     18373      C   /opt/conda/bin/python                       1317MiB |\n",
      "|    2     18373      C   /opt/conda/bin/python                       1317MiB |\n",
      "|    3     18373      C   /opt/conda/bin/python                       1317MiB |\n",
      "|    4     18373      C   /opt/conda/bin/python                       1317MiB |\n",
      "|    5     18373      C   /opt/conda/bin/python                       1317MiB |\n",
      "|    6     18373      C   /opt/conda/bin/python                       1317MiB |\n",
      "|    7     18373      C   /opt/conda/bin/python                       1317MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
