{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from google.cloud import storage\n",
    "import tokenizers\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18746072"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 128\n",
    "accum_multipler = 1\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "warmup_ratio = 0.06\n",
    "lr = 5e-4\n",
    "\n",
    "data_size = os.stat(\"/mnt/d/data_masked_%s\"%seq_length).st_size // (batch_size*4)\n",
    "\n",
    "num_batches = int(math.ceil(data_size / batch_size))\n",
    "tot_num_steps   = int(math.ceil((data_size / batch_size / accum_multipler)  * epochs))\n",
    "warmup_steps = int(tot_num_steps * warmup_ratio)\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_size:       3279552\n",
      "seq_length:      128\n",
      "lr:              0.0005\n",
      "epochs:          2\n",
      "tot_num_steps:   292906\n",
      "warmup_steps:    17574\n"
     ]
    }
   ],
   "source": [
    "print('num_batches:    ', num_batches)\n",
    "print('data_size:      ', data_size)\n",
    "print('seq_length:     ', seq_length)\n",
    "print('lr:             ', lr)\n",
    "print('epochs:         ', epochs)\n",
    "print('tot_num_steps:  ', tot_num_steps)\n",
    "print('warmup_steps:   ', warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():      \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('CPU')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(vocab_file = 'tokenizer/vocab.txt')\n",
    "tokenizer.add_special_tokens([\"<nl>\"])\n",
    "tokenizer.enable_truncation(max_length=seq_length)\n",
    "tokenizer.enable_padding(length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original_fn = \"/mnt/d/data_original_%s\"%seq_length\n",
    "data_masked_fn   = \"/mnt/d/data_masked_%s\"%seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "i = random.randint(0, 100000)\n",
    "with open(data_original_fn, \"rb\") as f:\n",
    "    data = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "    \n",
    "with open(data_masked_fn, \"rb\") as f:\n",
    "    data_masked = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] the full . she was watching him and he grinned at her . ‘ stop \u001b[31mfishing\u001b[0m for compliment ##s . ’ <nl> melan ##ie grinned back , her head on one side . ‘ so \u001b[31m,\u001b[0m what about you and fi ##zz beau ##mont then ? ’ <nl> it was his turn to hesitate . ‘ what \u001b[31mdo\u001b[0m you \u001b[31mmean\u001b[0m ? ’ <nl> she threw him a look full of misc ##hi ##ef \u001b[31m.\u001b[0m ‘ i asked her if \u001b[31myou\u001b[0m \u001b[31m’\u001b[0m d had \u001b[31ma\u001b[0m row or something . ’ <nl> ‘ \u001b[31mthat\u001b[0m \u001b[31mwas\u001b[0m very rude of you . ’ <nl> ‘ probably \u001b[31m.\u001b[0m but i wanted to know . ’ <nl> ‘ and what did she say \u001b[31m?\u001b[0m ’ <nl> melan ##ie ’ s eyes gle [SEP] \n",
      "\n",
      "[CLS] the full . she was watching him and he grinned at her . ‘ stop \u001b[34mfishing\u001b[0m for compliment ##s . ’ <nl> melan ##ie grinned back , her head on one side . ‘ so \u001b[34m,\u001b[0m what about you and fi ##zz beau ##mont then ? ’ <nl> it was his turn to hesitate . ‘ what \u001b[34mdo\u001b[0m you \u001b[34mmean\u001b[0m ? ’ <nl> she threw him a look full of misc ##hi ##ef \u001b[34m.\u001b[0m ‘ i asked her if \u001b[34myou\u001b[0m \u001b[34m’\u001b[0m d had \u001b[34ma\u001b[0m row or something . ’ <nl> ‘ \u001b[34mthat\u001b[0m \u001b[34mwas\u001b[0m very rude of you . ’ <nl> ‘ probably \u001b[34m.\u001b[0m but i wanted to know . ’ <nl> ‘ and what did she say \u001b[34m?\u001b[0m ’ <nl> melan ##ie ’ s eyes gle [SEP] \n",
      "\n",
      "[CLS] the full . she was watching him and he grinned at her . ‘ stop \u001b[31m[MASK]\u001b[0m for compliment ##s . ’ <nl> melan ##ie grinned back , her head on one side . ‘ so \u001b[31m[MASK]\u001b[0m what about you and fi ##zz beau ##mont then ? ’ <nl> it was his turn to hesitate . ‘ what \u001b[31m[MASK]\u001b[0m you \u001b[31m[MASK]\u001b[0m ? ’ <nl> she threw him a look full of misc ##hi ##ef \u001b[31m[MASK]\u001b[0m ‘ i asked her if \u001b[31m[MASK]\u001b[0m \u001b[31m##usion\u001b[0m d had \u001b[31m[MASK]\u001b[0m row or something . ’ <nl> ‘ \u001b[31m[MASK]\u001b[0m \u001b[31m[MASK]\u001b[0m very rude of you . ’ <nl> ‘ probably \u001b[31m[MASK]\u001b[0m but i wanted to know . ’ <nl> ‘ and what did she say \u001b[31m[MASK]\u001b[0m ’ <nl> melan ##ie ’ s eyes gle [SEP] "
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "tensor = torch.zeros(())\n",
    "labels = tensor.new_full(data.shape, -100).int()\n",
    "labels[data!=data_masked] = data[data!=data_masked]\n",
    "\n",
    "attention_mask = torch.where(data!=0, torch.ones_like(data), torch.zeros_like(data))\n",
    "\n",
    "for id, label in zip(data, labels):\n",
    "    token = tokenizer.id_to_token(id)\n",
    "    if label >= 0:\n",
    "        token = colored(token,'red')\n",
    "    print(token, end=\" \")\n",
    "print()\n",
    "print()\n",
    "\n",
    "for id, label in zip(data, labels):\n",
    "    if not id:\n",
    "        continue\n",
    "    token = tokenizer.id_to_token(id)\n",
    "    if label >= 0:\n",
    "        token = colored(tokenizer.id_to_token(label), 'blue')\n",
    "    print(token, end=\" \")\n",
    "print()\n",
    "print()\n",
    "for id, label in zip(data_masked, labels):\n",
    "    if not id:\n",
    "        continue\n",
    "    token = tokenizer.id_to_token(id)\n",
    "    if label >= 0:\n",
    "        token = colored(token,'red')\n",
    "    print(token, end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    def __getitem__(self,i):\n",
    "        with open(data_original_fn, \"rb\") as f:\n",
    "            data = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "\n",
    "        with open(data_masked_fn, \"rb\") as f:\n",
    "            data_masked = torch.tensor(np.fromfile(f,dtype=np.int32, count=seq_length, offset=seq_length*i*4))\n",
    "\n",
    "        tensor = torch.zeros(())\n",
    "        labels = tensor.new_full(data.shape, -100).int()\n",
    "        labels[data!=data_masked] = data[data!=data_masked]\n",
    "        \n",
    "        attention_mask = torch.where(data!=0, torch.ones_like(data), torch.zeros_like(data))\n",
    "        \n",
    "        return data_masked.long(), labels.long(), attention_mask.long(), data.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual batch size: 128\n",
      "Batch size per GPU per pass: 16\n"
     ]
    }
   ],
   "source": [
    "dataset = textDataset(data_size)\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "print('Actual batch size:', batch_size * accum_multipler)\n",
    "\n",
    "print('Batch size per GPU per pass:', batch_size // torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ElectraForPreTraining(\n",
       "    (electra): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(50000, 128, padding_idx=0)\n",
       "        (position_embeddings): Embedding(128, 128)\n",
       "        (token_type_embeddings): Embedding(2, 128)\n",
       "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (discriminator_predictions): ElectraDiscriminatorPredictions(\n",
       "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (dense_prediction): Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ElectraForMaskedLM, ElectraForPreTraining\n",
    "from transformers import ElectraConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "generator_config = ElectraConfig(\n",
    "    max_position_embeddings=seq_length,\n",
    "    num_hidden_layers=12,\n",
    "    vocab_size=50000,\n",
    "    embedding_size=128,\n",
    "    hidden_size = 64,\n",
    "    intermediate_size = 256,\n",
    "    num_attention_heads=1,\n",
    ")\n",
    "discriminator_config = ElectraConfig(\n",
    "    max_position_embeddings=seq_length,\n",
    "    num_hidden_layers=12,\n",
    "    vocab_size=50000,\n",
    "    embedding_size=128,\n",
    "    hidden_size=256,\n",
    "    intermediate_size=1024,\n",
    "    num_attention_heads=4,\n",
    ")\n",
    "\n",
    "generator = nn.DataParallel(ElectraForMaskedLM(config=generator_config))\n",
    "generator.to(device)\n",
    "discriminator = nn.DataParallel(ElectraForPreTraining(config=discriminator_config))\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.module.electra.embeddings = generator.module.electra.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AdamW\n",
    "generator_optimizer = AdamW(\n",
    "    generator.parameters(), betas=(0.9, 0.999), \n",
    "    lr = lr, \n",
    "    weight_decay=0.01)\n",
    "discriminator_optimizer = AdamW(\n",
    "    discriminator.parameters(), betas=(0.9, 0.999), \n",
    "    lr = lr, \n",
    "    weight_decay=0.01)\n",
    "\n",
    "total_steps = len(dataloader) * epochs\n",
    "generator_scheduler = get_linear_schedule_with_warmup(generator_optimizer, \n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = tot_num_steps)\n",
    "discriminator_scheduler = get_linear_schedule_with_warmup(discriminator_optimizer, \n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = tot_num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of  25,622.    Elapsed: 0:03:17.    Generator Loss: 54.284.    Discriminator Loss: 2.914.\n",
      "  Batch   400  of  25,622.    Elapsed: 0:06:02.    Generator Loss: 52.188.    Discriminator Loss: 1.462.\n",
      "  Batch   600  of  25,622.    Elapsed: 0:08:48.    Generator Loss: 48.621.    Discriminator Loss: 0.640.\n",
      "  Batch   800  of  25,622.    Elapsed: 0:11:35.    Generator Loss: 44.140.    Discriminator Loss: 0.409.\n",
      "  Batch 1,000  of  25,622.    Elapsed: 0:14:24.    Generator Loss: 40.509.    Discriminator Loss: 0.550.\n",
      "  Batch 1,200  of  25,622.    Elapsed: 0:17:08.    Generator Loss: 38.512.    Discriminator Loss: 0.437.\n",
      "  Batch 1,400  of  25,622.    Elapsed: 0:19:54.    Generator Loss: 37.989.    Discriminator Loss: 0.432.\n",
      "  Batch 1,600  of  25,622.    Elapsed: 0:22:44.    Generator Loss: 37.774.    Discriminator Loss: 0.426.\n",
      "  Batch 1,800  of  25,622.    Elapsed: 0:25:30.    Generator Loss: 37.747.    Discriminator Loss: 0.428.\n",
      "  Batch 2,000  of  25,622.    Elapsed: 0:28:16.    Generator Loss: 37.744.    Discriminator Loss: 0.425.\n",
      "  Batch 2,200  of  25,622.    Elapsed: 0:31:01.    Generator Loss: 37.709.    Discriminator Loss: 0.420.\n",
      "  Batch 2,400  of  25,622.    Elapsed: 0:33:48.    Generator Loss: 37.709.    Discriminator Loss: 0.415.\n",
      "  Batch 2,600  of  25,622.    Elapsed: 0:36:35.    Generator Loss: 37.077.    Discriminator Loss: 0.573.\n",
      "  Batch 2,800  of  25,622.    Elapsed: 0:39:20.    Generator Loss: 35.639.    Discriminator Loss: 0.604.\n",
      "  Batch 3,000  of  25,622.    Elapsed: 0:42:02.    Generator Loss: 35.207.    Discriminator Loss: 0.586.\n",
      "  Batch 3,200  of  25,622.    Elapsed: 0:44:47.    Generator Loss: 35.045.    Discriminator Loss: 0.571.\n",
      "  Batch 3,400  of  25,622.    Elapsed: 0:47:33.    Generator Loss: 34.965.    Discriminator Loss: 0.605.\n",
      "  Batch 3,600  of  25,622.    Elapsed: 0:50:17.    Generator Loss: 34.749.    Discriminator Loss: 0.620.\n",
      "  Batch 3,800  of  25,622.    Elapsed: 0:53:03.    Generator Loss: 34.586.    Discriminator Loss: 0.611.\n",
      "  Batch 4,000  of  25,622.    Elapsed: 0:55:49.    Generator Loss: 34.507.    Discriminator Loss: 0.617.\n",
      "  Batch 4,200  of  25,622.    Elapsed: 0:58:33.    Generator Loss: 34.294.    Discriminator Loss: 0.593.\n",
      "  Batch 4,400  of  25,622.    Elapsed: 1:01:20.    Generator Loss: 34.142.    Discriminator Loss: 0.572.\n",
      "  Batch 4,600  of  25,622.    Elapsed: 1:04:04.    Generator Loss: 34.045.    Discriminator Loss: 0.573.\n",
      "  Batch 4,800  of  25,622.    Elapsed: 1:06:50.    Generator Loss: 33.987.    Discriminator Loss: 0.579.\n",
      "  Batch 5,000  of  25,622.    Elapsed: 1:09:36.    Generator Loss: 33.863.    Discriminator Loss: 0.580.\n",
      "  Batch 5,200  of  25,622.    Elapsed: 1:12:24.    Generator Loss: 33.870.    Discriminator Loss: 0.587.\n",
      "  Batch 5,400  of  25,622.    Elapsed: 1:15:09.    Generator Loss: 33.825.    Discriminator Loss: 0.593.\n",
      "  Batch 5,600  of  25,622.    Elapsed: 1:17:56.    Generator Loss: 33.750.    Discriminator Loss: 0.597.\n",
      "  Batch 5,800  of  25,622.    Elapsed: 1:20:42.    Generator Loss: 33.774.    Discriminator Loss: 0.595.\n",
      "  Batch 6,000  of  25,622.    Elapsed: 1:23:29.    Generator Loss: 33.707.    Discriminator Loss: 0.607.\n",
      "  Batch 6,200  of  25,622.    Elapsed: 1:26:15.    Generator Loss: 33.674.    Discriminator Loss: 0.605.\n",
      "  Batch 6,400  of  25,622.    Elapsed: 1:29:02.    Generator Loss: 33.630.    Discriminator Loss: 0.604.\n",
      "  Batch 6,600  of  25,622.    Elapsed: 1:31:47.    Generator Loss: 33.593.    Discriminator Loss: 0.613.\n",
      "  Batch 6,800  of  25,622.    Elapsed: 1:34:32.    Generator Loss: 33.547.    Discriminator Loss: 0.616.\n",
      "  Batch 7,000  of  25,622.    Elapsed: 1:37:18.    Generator Loss: 33.483.    Discriminator Loss: 0.614.\n",
      "  Batch 7,200  of  25,622.    Elapsed: 1:40:03.    Generator Loss: 33.482.    Discriminator Loss: 0.615.\n",
      "  Batch 7,400  of  25,622.    Elapsed: 1:42:50.    Generator Loss: 33.486.    Discriminator Loss: 0.612.\n",
      "  Batch 7,600  of  25,622.    Elapsed: 1:45:38.    Generator Loss: 33.425.    Discriminator Loss: 0.611.\n",
      "  Batch 7,800  of  25,622.    Elapsed: 1:48:25.    Generator Loss: 33.370.    Discriminator Loss: 0.608.\n",
      "  Batch 8,000  of  25,622.    Elapsed: 1:51:11.    Generator Loss: 33.364.    Discriminator Loss: 0.608.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_t0 = time.time()\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    generator_train_loss = 0\n",
    "    discriminator_train_loss = 0\n",
    "\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    generator.zero_grad()\n",
    "    discriminator.zero_grad()\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        #generator\n",
    "        generator_input = batch[0].to(device)\n",
    "        generator_labels = batch[1].to(device)\n",
    "        generator_mask = batch[2].to(device)\n",
    "        generator_original = batch[3].to(device)\n",
    "        \n",
    "        generator_loss, generator_scores = generator(generator_input, attention_mask=generator_mask, labels=generator_labels)\n",
    "        generator_loss = generator_loss.mean()\n",
    "        generator_train_loss += generator_loss.item()\n",
    "        generator_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), 1.0)\n",
    "        \n",
    "        #discriminator\n",
    "        discriminator_input = torch.where(generator_labels>=0, torch.argmax(generator_scores,dim=2), generator_original)\n",
    "        discriminator_labels = torch.where(discriminator_input==generator_original, \n",
    "                                           torch.zeros_like(generator_original), torch.ones_like(generator_original))\n",
    "        discriminator_mask = generator_mask\n",
    "        \n",
    "        \n",
    "        discriminator_loss, discriminator_scores = discriminator(discriminator_input, \n",
    "                                                    attention_mask=discriminator_mask, labels=discriminator_labels)\n",
    "        discriminator_loss = discriminator_loss.mean()\n",
    "        discriminator_train_loss += discriminator_loss.item()\n",
    "        discriminator_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), 1.0)\n",
    "        \n",
    "        if step % accum_multipler == 0 and (accum_multipler == 1 or step != 0):\n",
    "            generator_optimizer.step()\n",
    "            generator_scheduler.step()\n",
    "            discriminator_optimizer.step()\n",
    "            discriminator_scheduler.step()\n",
    "            generator.zero_grad()\n",
    "            discriminator.zero_grad()\n",
    "        \n",
    "        if step % 200 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.    Generator Loss: {:.3f}.    Discriminator Loss: {:.3f}.'\n",
    "                  .format(step, \n",
    "                          len(dataloader), \n",
    "                          elapsed, \n",
    "                          generator_train_loss/200, discriminator_train_loss/200))\n",
    "            generator_train_loss = 0\n",
    "            discriminator_train_loss = 0\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator,'electra_small_generator_%s.pth'%seq_length)\n",
    "torch.save(discriminator,'electra_small_discriminator_%s.pth'%seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
